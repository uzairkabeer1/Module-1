{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = df[col].apply(lambda x: tuple(map(float, x.strip('()').split(','))))\n",
    "\n",
    "    \n",
    "    labels = df['Label']\n",
    "    features = df.drop(columns=['Label'])\n",
    "\n",
    "    \n",
    "    for i in range(1, 22):\n",
    "        features[f'Keypoint_{i}_x'] = features[f'Keypoint_{i}'].apply(lambda x: x[0])\n",
    "        features[f'Keypoint_{i}_y'] = features[f'Keypoint_{i}'].apply(lambda x: x[1])\n",
    "\n",
    "    \n",
    "    features.drop(columns=df.columns[1:22], inplace=True)\n",
    "\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, model_filename):\n",
    "    joblib.dump(model, model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_models(features, labels):\n",
    "    X = features\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    label_encoder_filename = '../label_encoder.joblib'\n",
    "    save_model(label_encoder, label_encoder_filename)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    classifiers = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'SVM': SVC(kernel='linear', random_state=42),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'GBM': GradientBoostingClassifier(random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Neural Network': MLPClassifier(max_iter=1000, random_state=42)\n",
    "    }\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(report)\n",
    "        model_filename = f'../trained_{name.replace(\" \", \"_\")}.joblib'\n",
    "        save_model(clf, model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       106\n",
      "           1       0.98      1.00      0.99       125\n",
      "           2       0.98      0.98      0.98       128\n",
      "           3       0.95      0.94      0.95       124\n",
      "           4       0.95      0.95      0.95       131\n",
      "           5       0.99      0.99      0.99       164\n",
      "           6       0.97      0.97      0.97       148\n",
      "           7       0.97      0.99      0.98       142\n",
      "           8       1.00      0.99      0.99        94\n",
      "           9       0.93      0.99      0.96        92\n",
      "          10       0.99      0.98      0.98       131\n",
      "          11       1.00      0.99      0.99       134\n",
      "          12       0.89      0.90      0.90        84\n",
      "          13       0.98      0.98      0.98       126\n",
      "          14       0.89      0.96      0.93        97\n",
      "          15       0.91      0.93      0.92        68\n",
      "          16       1.00      0.90      0.95        30\n",
      "          17       0.99      0.96      0.97       154\n",
      "          18       0.96      0.87      0.91        54\n",
      "          19       0.96      0.96      0.96        95\n",
      "          20       0.99      0.98      0.98       142\n",
      "          21       0.95      0.97      0.96       144\n",
      "          22       0.97      0.96      0.96       138\n",
      "          23       0.94      0.97      0.96       106\n",
      "          24       1.00      0.98      0.99       111\n",
      "          25       0.97      0.98      0.97        98\n",
      "\n",
      "    accuracy                           0.97      2966\n",
      "   macro avg       0.96      0.96      0.96      2966\n",
      "weighted avg       0.97      0.97      0.97      2966\n",
      "\n",
      "Training Decision Tree...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       106\n",
      "           1       0.95      0.99      0.97       125\n",
      "           2       0.95      0.98      0.96       128\n",
      "           3       0.91      0.88      0.89       124\n",
      "           4       0.97      0.91      0.94       131\n",
      "           5       0.99      0.99      0.99       164\n",
      "           6       0.89      0.93      0.91       148\n",
      "           7       0.95      0.96      0.96       142\n",
      "           8       0.98      0.94      0.96        94\n",
      "           9       0.91      0.89      0.90        92\n",
      "          10       0.95      0.95      0.95       131\n",
      "          11       0.94      0.96      0.95       134\n",
      "          12       0.87      0.87      0.87        84\n",
      "          13       0.92      0.97      0.95       126\n",
      "          14       0.92      0.91      0.91        97\n",
      "          15       0.79      0.79      0.79        68\n",
      "          16       0.96      0.80      0.87        30\n",
      "          17       0.96      0.95      0.95       154\n",
      "          18       0.75      0.67      0.71        54\n",
      "          19       0.85      0.91      0.88        95\n",
      "          20       0.97      0.96      0.96       142\n",
      "          21       0.94      0.94      0.94       144\n",
      "          22       0.96      0.95      0.95       138\n",
      "          23       0.91      0.92      0.92       106\n",
      "          24       0.95      0.95      0.95       111\n",
      "          25       0.93      0.95      0.94        98\n",
      "\n",
      "    accuracy                           0.93      2966\n",
      "   macro avg       0.92      0.92      0.92      2966\n",
      "weighted avg       0.93      0.93      0.93      2966\n",
      "\n",
      "Training SVM...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       106\n",
      "           1       0.96      1.00      0.98       125\n",
      "           2       0.98      0.99      0.99       128\n",
      "           3       0.89      0.94      0.91       124\n",
      "           4       0.90      0.94      0.92       131\n",
      "           5       0.99      0.99      0.99       164\n",
      "           6       0.97      0.95      0.96       148\n",
      "           7       0.96      0.99      0.98       142\n",
      "           8       1.00      0.96      0.98        94\n",
      "           9       0.90      0.97      0.93        92\n",
      "          10       1.00      0.98      0.99       131\n",
      "          11       0.97      0.99      0.98       134\n",
      "          12       0.95      0.86      0.90        84\n",
      "          13       0.98      0.98      0.98       126\n",
      "          14       0.84      0.97      0.90        97\n",
      "          15       0.84      0.90      0.87        68\n",
      "          16       0.96      0.90      0.93        30\n",
      "          17       0.99      0.94      0.97       154\n",
      "          18       0.96      0.81      0.88        54\n",
      "          19       0.88      0.96      0.92        95\n",
      "          20       0.99      0.98      0.98       142\n",
      "          21       0.93      0.99      0.96       144\n",
      "          22       1.00      0.92      0.96       138\n",
      "          23       0.94      0.87      0.90       106\n",
      "          24       0.98      0.98      0.98       111\n",
      "          25       0.96      0.99      0.97        98\n",
      "\n",
      "    accuracy                           0.95      2966\n",
      "   macro avg       0.95      0.95      0.95      2966\n",
      "weighted avg       0.96      0.95      0.95      2966\n",
      "\n",
      "Training KNN...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       106\n",
      "           1       0.98      0.99      0.98       125\n",
      "           2       0.97      0.99      0.98       128\n",
      "           3       0.90      0.90      0.90       124\n",
      "           4       0.87      0.98      0.92       131\n",
      "           5       0.96      0.99      0.98       164\n",
      "           6       0.97      0.97      0.97       148\n",
      "           7       0.98      0.99      0.99       142\n",
      "           8       1.00      0.97      0.98        94\n",
      "           9       0.90      0.99      0.94        92\n",
      "          10       1.00      1.00      1.00       131\n",
      "          11       1.00      0.99      1.00       134\n",
      "          12       0.93      0.75      0.83        84\n",
      "          13       0.95      0.95      0.95       126\n",
      "          14       0.95      0.95      0.95        97\n",
      "          15       0.95      0.84      0.89        68\n",
      "          16       1.00      0.93      0.97        30\n",
      "          17       1.00      0.97      0.98       154\n",
      "          18       0.90      0.80      0.84        54\n",
      "          19       0.91      0.97      0.94        95\n",
      "          20       1.00      0.99      1.00       142\n",
      "          21       0.95      0.98      0.97       144\n",
      "          22       0.98      0.95      0.97       138\n",
      "          23       0.87      0.92      0.89       106\n",
      "          24       1.00      0.97      0.99       111\n",
      "          25       0.98      0.98      0.98        98\n",
      "\n",
      "    accuracy                           0.96      2966\n",
      "   macro avg       0.96      0.95      0.95      2966\n",
      "weighted avg       0.96      0.96      0.96      2966\n",
      "\n",
      "Training GBM...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       106\n",
      "           1       0.98      1.00      0.99       125\n",
      "           2       0.99      0.98      0.98       128\n",
      "           3       0.97      0.94      0.95       124\n",
      "           4       0.95      0.95      0.95       131\n",
      "           5       0.98      0.99      0.98       164\n",
      "           6       0.93      0.97      0.95       148\n",
      "           7       0.97      0.99      0.98       142\n",
      "           8       0.97      0.99      0.98        94\n",
      "           9       0.93      0.93      0.93        92\n",
      "          10       0.99      0.96      0.98       131\n",
      "          11       0.98      0.98      0.98       134\n",
      "          12       0.89      0.87      0.88        84\n",
      "          13       0.98      0.98      0.98       126\n",
      "          14       0.93      0.96      0.94        97\n",
      "          15       0.81      0.87      0.84        68\n",
      "          16       0.96      0.83      0.89        30\n",
      "          17       0.99      0.97      0.98       154\n",
      "          18       0.81      0.89      0.85        54\n",
      "          19       0.94      0.95      0.94        95\n",
      "          20       0.96      0.96      0.96       142\n",
      "          21       0.95      0.96      0.96       144\n",
      "          22       0.98      0.95      0.97       138\n",
      "          23       0.91      0.99      0.95       106\n",
      "          24       1.00      0.95      0.97       111\n",
      "          25       0.97      0.97      0.97        98\n",
      "\n",
      "    accuracy                           0.96      2966\n",
      "   macro avg       0.95      0.95      0.95      2966\n",
      "weighted avg       0.96      0.96      0.96      2966\n",
      "\n",
      "Training Logistic Regression...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92       106\n",
      "           1       0.97      1.00      0.98       125\n",
      "           2       0.97      0.98      0.98       128\n",
      "           3       0.86      0.92      0.89       124\n",
      "           4       0.85      0.94      0.89       131\n",
      "           5       0.98      0.99      0.98       164\n",
      "           6       0.97      0.96      0.96       148\n",
      "           7       0.98      0.99      0.99       142\n",
      "           8       0.97      0.98      0.97        94\n",
      "           9       0.87      0.96      0.91        92\n",
      "          10       0.95      0.95      0.95       131\n",
      "          11       0.93      0.99      0.96       134\n",
      "          12       0.97      0.74      0.84        84\n",
      "          13       0.87      0.98      0.92       126\n",
      "          14       0.93      0.89      0.91        97\n",
      "          15       0.80      0.81      0.80        68\n",
      "          16       0.86      0.83      0.85        30\n",
      "          17       0.97      0.91      0.94       154\n",
      "          18       0.67      0.19      0.29        54\n",
      "          19       0.76      0.98      0.86        95\n",
      "          20       0.87      0.96      0.91       142\n",
      "          21       0.95      0.94      0.94       144\n",
      "          22       1.00      0.93      0.97       138\n",
      "          23       0.93      0.86      0.89       106\n",
      "          24       1.00      0.98      0.99       111\n",
      "          25       0.95      0.99      0.97        98\n",
      "\n",
      "    accuracy                           0.93      2966\n",
      "   macro avg       0.92      0.90      0.90      2966\n",
      "weighted avg       0.93      0.93      0.92      2966\n",
      "\n",
      "Training Neural Network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93       106\n",
      "           1       0.98      1.00      0.99       125\n",
      "           2       0.98      0.99      0.98       128\n",
      "           3       0.97      0.98      0.97       124\n",
      "           4       0.99      0.97      0.98       131\n",
      "           5       1.00      0.99      0.99       164\n",
      "           6       0.98      0.97      0.97       148\n",
      "           7       0.98      0.99      0.99       142\n",
      "           8       0.98      0.98      0.98        94\n",
      "           9       0.95      0.99      0.97        92\n",
      "          10       1.00      1.00      1.00       131\n",
      "          11       1.00      0.99      0.99       134\n",
      "          12       0.90      0.93      0.91        84\n",
      "          13       0.98      0.98      0.98       126\n",
      "          14       0.95      0.97      0.96        97\n",
      "          15       0.92      0.96      0.94        68\n",
      "          16       0.97      0.93      0.95        30\n",
      "          17       1.00      0.97      0.98       154\n",
      "          18       0.93      0.94      0.94        54\n",
      "          19       0.93      0.98      0.95        95\n",
      "          20       1.00      0.98      0.99       142\n",
      "          21       0.99      0.98      0.98       144\n",
      "          22       0.99      0.99      0.99       138\n",
      "          23       0.95      0.99      0.97       106\n",
      "          24       1.00      0.99      1.00       111\n",
      "          25       0.99      0.98      0.98        98\n",
      "\n",
      "    accuracy                           0.98      2966\n",
      "   macro avg       0.97      0.97      0.97      2966\n",
      "weighted avg       0.98      0.98      0.98      2966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_csv_path = '../train_data.csv'\n",
    "test_csv_path = '../test_data.csv'\n",
    "\n",
    "train_features, train_labels = preprocess_dataset(train_csv_path)\n",
    "test_features, test_labels = preprocess_dataset(test_csv_path)\n",
    "\n",
    "train_multiple_models(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
